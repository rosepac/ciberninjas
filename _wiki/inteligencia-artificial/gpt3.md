---
title: "GPT 3"
description: "üë®‚Äçüíª WikiNinjas, la Enciclopedia Inform√°tica Tecnol√≥gica Ciberninjas: GPT 3, es una m√°quina capaz de transformador y generar contenidos, si es preentrenado con anterioridad."
excerpt: "üë®‚Äçüíª WikiNinjas, la Enciclopedia Inform√°tica Tecnol√≥gica Ciberninjas: GPT 3, es una m√°quina capaz de transformador y generar contenidos, si es preentrenado con anterioridad."
date: 2020-08-11 14:58:30
toc: true
toc_label: "Contenidos"
toc_icon: user-ninja
toc_sticky: true
published: true
author: rosepac
bootstrap: true
layout: post
permalink: /wiki/gpt3/
canonical_URL: https://ciberninjas.com/wiki/gpt3/
feature-img: /assets/img/wiki/articulos/wiki-gpt3.webp
img: /assets/img/wiki/articulos/wiki-gpt3.webp
---

GPT-3, es un modelo de lenguaje autorregresivo que utiliza el aprendizaje profundo para producir texto similar al de un humano. Es el modelo de predicci√≥n de lenguaje de tercera generaci√≥n de la serie GPT-n creada por OpenAI, un laboratorio de investigaci√≥n de inteligencia artificial con sede en San Francisco. La versi√≥n completa de GPT-3 tiene una capacidad de 175 mil millones de par√°metros de aprendizaje autom√°tico, que es m√°s de dos √≥rdenes de magnitud mayor que la de su predecesor, GPT-2.

GPT-3, que se introdujo en Mayo de 2020 y se encuentra en prueba beta a partir de julio de 2020, es parte de una tendencia en los sistemas de procesamiento del lenguaje natural (NLP) de representaciones de lenguaje previamente entrenadas. Antes del lanzamiento de GPT-3, el modelo de lenguaje m√°s grande era Turing NLG de Microsoft, presentado en febrero de 2020, con una capacidad diez veces menor que la de GPT-3.

La calidad del texto generado por GPT-3 es tan alta que es dif√≠cil distinguirlo del escrito por un humano, que tiene tanto beneficios como riesgos. Treinta y un investigadores e ingenieros de OpenAI presentaron el documento original del 28 de mayo de 2020 que presenta el GPT-3. En su art√≠culo, advirtieron sobre los peligros potenciales de GPT-3 y pidieron investigaci√≥n para mitigar el riesgo: David Chalmers , un fil√≥sofo australiano, describi√≥ al GPT-3 como "uno de los sistemas de inteligencia artificial m√°s interesantes e importantes jam√°s producidos".

## **Antecedentes**

Seg√∫n The Economist, los algoritmos mejorados, las computadoras potentes y el aumento de los datos digitalizados han impulsado una revoluci√≥n en el aprendizaje autom√°tico , con nuevas t√©cnicas en la d√©cada de 2010 que dieron como resultado "mejoras r√°pidas en las tareas", incluida la manipulaci√≥n del lenguaje.

Los modelos de software est√°n entrenados para aprender utilizando miles o millones de ejemplos en una "estructura  ... basada libremente en la arquitectura neuronal del cerebro".

La arquitectura m√°s utilizada en el procesamiento del lenguaje natural (PNL) es una red neuronal. Se basa en un modelo de aprendizaje profundo que se introdujo por primera vez en 2017: elmodelo de aprendizaje autom√°tico de transformadores.

Los modelos GPT-n se basan en esta arquitectura de red neuronal basada en transformadores de aprendizaje profundo. Hay una serie de sistemas de PNL capaces de procesar, extraer, organizar, conectar, contrastar, comprender y generar respuestas a preguntas.

El 11 de junio de 2018, los investigadores e ingenieros de OpenAI publicaron su art√≠culo original sobre modelos generativos, modelos de lenguaje, sistemas de inteligencia artificial, que podr√≠an ser entrenados previamente con un corpus enorme y diverso de texto a trav√©s de conjuntos de datos, en un proceso que llamaron pre-generativo. entrenamiento (GP).

Los autores describieron c√≥mo se mejoraron los desempe√±os de comprensi√≥n del lenguaje en el procesamiento del lenguaje natural (NLP) en el transformador-n previo al entrenamiento generativo (GPT-n) a trav√©s de un proceso de "entrenamiento previo generativo de un modelo de lenguaje en un corpus diverso de texto sin etiquetar, seguido de un ajuste fino discriminativo en cada tarea espec√≠fica ".

Esto elimin√≥ la necesidad de supervisi√≥n humana y de etiquetado manual que requiere mucho tiempo.

En febrero de 2020, Microsoft present√≥ su Turing Natural Language Generation (T-NLG), que entonces era el "modelo de lenguaje m√°s grande jam√°s publicado con 17 mil millones de par√°metros". Funcion√≥ mejor que cualquier otro modelo de lenguaje en una variedad de tareas que inclu√≠an resumir textos y responder preguntas.

## **Capacidad de GPT 3**

Una preimpresi√≥n de arXiv del 28 de mayo de 2020 de un grupo de 31 ingenieros e investigadores de OpenAI describi√≥ el desarrollo de un "modelo de lenguaje de √∫ltima generaci√≥n" llamado GPT-3 o Generative Pretrained Transformer 3, un modelo de lenguaje de tercera generaci√≥n. El equipo hab√≠a logrado aumentar la capacidad de GPT-3 en m√°s de dos √≥rdenes de magnitud con respecto a la de su predecesor, GPT-2, lo que convirti√≥ a GPT-3 en el modelo de lenguaje no disperso m√°s grande hasta la fecha.

El mayor n√∫mero de par√°metros de GPT-3 le otorga un mayor nivel de precisi√≥n en relaci√≥n con versiones anteriores con menor capacidad. La capacidad de GPT-3 es diez veces mayor que la de Turing NLG de Microsoft.

El sesenta por ciento del conjunto de datos de preentrenamiento ponderado para GPT-3 proviene de una versi√≥n filtrada de Common Crawl que consta de 410 mil millones de tokens codificados por pares de bytes, otras fuentes son 19 mil millones de tokens de WebText2 que representan el 22% del total ponderado, 12 mil millones de tokens de Books que representan el 8%, 55 mil millones de tokens de Books2 que representan el 8% y 3 mil millones de tokens de Wikipedia que representan el 3%.

GPT-3 fue entrenado en cientos de miles de millones de palabras y es capaz de codificar en CSS, JSX, Python, entre otros. Dado que los datos de entrenamiento de GPT-3 eran completos, no requiere m√°s entrenamiento para distintas tareas de lenguaje.

El 11 de junio de 2020, OpenAI anunci√≥ que los usuarios pod√≠an solicitar acceso a su API GPT-3 f√°cil de usar, un "conjunto de herramientas de aprendizaje autom√°tico", para ayudar a OpenAI a "explorar las fortalezas y los l√≠mites" de esta nueva tecnolog√≠a. La invitaci√≥n describ√≠a c√≥mo esta API ten√≠a una interfaz de "entrada de texto, salida de texto" de prop√≥sito general que puede completar casi "cualquier tarea en ingl√©s", en lugar del caso de uso √∫nico habitual.

Seg√∫n un usuario, que ten√≠a acceso a una versi√≥n inicial privada de la API OpenAI GPT-3, GPT-3 era "inquietantemente bueno" para escribir "texto incre√≠blemente coherente" con solo unas pocas indicaciones simples.

Debido a que GPT-3 puede "generar art√≠culos de noticias que los evaluadores humanos tienen dificultades para distinguir de los art√≠culos escritos por humanos", GPT-3 tiene el "potencial de promover tanto las aplicaciones beneficiosas como las da√±inas de los modelos de lenguaje".

En su art√≠culo del 28 de mayo de 2020, los investigadores describieron en detalle los posibles "efectos da√±inos de GPT-3" que incluyen "informaci√≥n err√≥nea, spam, phishing, abuso de procesos legales y gubernamentales, ensayo acad√©mico fraudulento pretextos de redacci√≥n e ingenier√≠a social ".

Los autores llaman la atenci√≥n sobre estos peligros para llamar a la investigaci√≥n sobre la mitigaci√≥n de riesgos.

## **Rese√±as de GPT 3**

En su revisi√≥n del 29 de julio de 2020 en The New York Times , Farhad Manjoo dijo que GPT-3, que puede generar c√≥digo de computadora y poes√≠a, as√≠ como prosa, no es solo "asombroso", "espeluznante" y "humillante", pero tambi√©n "m√°s que un poco aterrador".

Daily Nous present√≥ una serie de art√≠culos de nueve fil√≥sofos sobre GPT-3. El fil√≥sofo australiano David Chalmers describi√≥ al GPT-3 como "uno de los sistemas de IA m√°s interesantes e importantes jam√°s producidos".

Una revisi√≥n en Wired dijo que GPT-3 estaba "provocando escalofr√≠os en Silicon Valley".

Un art√≠culo en Towards Data Science declar√≥ que GPT-3 se entren√≥ en cientos de miles de millones de palabras y es capaz de codificar en CSS, JSX, Python y otros lenguajes.

La National Law Review dijo que GPT-3 es un "paso impresionante en el proceso m√°s grande", con OpenAI y otros encontrando "aplicaciones √∫tiles para todo este poder" mientras contin√∫an "trabajando hacia una inteligencia m√°s general".

<!-- Comunidades de Mujeres en el Mundo de la Tecnolog√≠a https://en.wikipedia.org/wiki/Category:Organizations_for_women_in_science_and_technology -->
**Autor**: Open AI
{: .notice--primary}

**Versi√≥n Inicial**: 11 de junio de 2020 (beta)
{: .notice--primary}

**Repositorio**: [Open AI](https://github.com/openai/gpt-3)
{: .notice--primary}

**Sitio Web**: [Open AI P√°gina web](https://openai.com/blog/openai-api/){:target="_blank" rel="nofollow,noreferrer"}
{: .notice--primary}

**Categor√≠as**: Aprendizaje Autom√°tico \ Aprendizaje Profundo \ Inteligencia Artificial
{: .notice--success}

<!-- https://en.wikipedia.org/wiki/Category:Organizations_for_women_in_science_and_technology https://www.techrepublic.com/article/10-awesome-technology-nonprofits-you-should-know-about/ https://en.wikipedia.org/wiki/Nonprofit_Technology_Resources -->
**INF.**: Esta obra contiene una traducci√≥n total derivada de [GPT 3](https://en.wikipedia.org/wiki/GPT-3){:target="_blank" rel="nofollow,noreferrer"} de la Wikipedia en ingl√©s, versi√≥n del 10 de Agosto de 2020, publicada por [sus editores](https://en.wikipedia.org/w/index.php?title=GPT-3&action=history){:target="_blank" rel="nofollow,noreferrer"} bajo la Licencia Libre de [GNU](http://www.gnu.org/licenses/licenses.html#GPL){:target="_blank" rel="nofollow,noreferrer"} [(es)](https://es.wikipedia.org/wiki/Wikipedia:Traducci%C3%B3n_no_oficial_de_la_Licencia_de_documentaci%C3%B3n_libre_de_GNU){:target="_blank" rel="nofollow,noreferrer"} y licencia [CC BY 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.es){:target="_blank" rel="nofollow,noreferrer"}.
{: .notice--info}

<!-- organizaciones de mujeres : https://en.wikipedia.org/wiki/Category:Organizations_for_women_in_science_and_technology https://en.wikipedia.org/wiki/Women_Who_Code -->