---

author: rosepac
bootstrap: true
comments: false
tags:
- Historia Ordenadores
categories:
- GOCT
thumbnail: /assets/img/blog/
title: 'Breve historia de los ordenadores'
description: >-
  Una historia de computadoras fÃ¡cil de entender, desde el Ã¡baco hasta Internet y iPhone.
excerpt: >-
  Una historia de computadoras fÃ¡cil de entender, desde el Ã¡baco hasta Internet y iPhone.
canonical_URL: https://ciberninjas.com/historia-ordenadores/
permalink: /historia-ordenadores/
date: 2020-05-08 00:32:32
last_modified_at: 
published: false

---

![](/assets/img/ "")

Las computadoras realmente se convirtieron en grandes inventos en las Ãºltimas dos dÃ©cadas del siglo XX. Pero su historia se remonta mÃ¡s de 2500 aÃ±os atrÃ¡s al Ã¡baco: una calculadora simple hecha de cuentas y alambres, que todavÃ­a se usa en algunas partes del mundo en la actualidad. La diferencia entre un Ã¡baco antiguo y una computadora moderna parece enorme, pero el principio, hacer cÃ¡lculos repetidos mÃ¡s rÃ¡pidamente que el cerebro humano, es exactamente el mismo.

> ğŸ”¥ Seguro tambiÃ©n te interesa: [mejores libros de programaciÃ³n](/programar/) >> [programas para desarrolladores](/mejores-sistemas-operativos-para-hackear/) >> [mejores lenguajes de programaciÃ³n](/15-mejores-lenguajes-programacion/) >> [portÃ¡tiles para programadores]() >> [mejores auriculares para programadores](/auriculares-dise%C3%B1o/) >> [ratones verticales para trabajar](/teclados-ratones-dise%C3%B1o/) >> [componentes de PC para programadores](/ordenadores-componentes/) >> [mejores regalos 1 - ğŸ“¦](/black-friday-amazon/) >> [mejores regalos 2 - ğŸ](/prime-day-amazon/)
{: .notice--danger}

- [1. **Ãbacos y calculadoras**](#1-%c3%81bacos-y-calculadoras)
- [2. **Motores de cÃ¡lculo**](#2-motores-de-c%c3%a1lculo)
- [3. **Bush y la bomba**](#3-bush-y-la-bomba)
- [4. **Turing: Probando**](#4-turing-probando)
- [5. **Las primeras computadoras modernas**](#5-las-primeras-computadoras-modernas)
- [6. **La revoluciÃ³n micro electrÃ³nica**](#6-la-revoluci%c3%b3n-micro-electr%c3%b3nica)
- [7. **Computadoras personales**](#7-computadoras-personales)
- [8. **La revoluciÃ³n del usuario**](#8-la-revoluci%c3%b3n-del-usuario)
- [9. **De las redes a internet**](#9-de-las-redes-a-internet)
- [10. **Â¿QuÃ© nos espera en el futuro?**](#10-%c2%bfqu%c3%a9-nos-espera-en-el-futuro)
  - [Relacionados](#relacionados)
<!-- https://www.explainthatstuff.com/historyofcomputers.html -->

## 1. **Ãbacos y calculadoras**

Es una medida del brillo del Ã¡baco, inventado en el Medio Oriente alrededor del aÃ±o 500 a. C., que siguiÃ³ siendo la forma mÃ¡s rÃ¡pida de calculadora hasta mediados del siglo XVII.

Luego, en 1642, con solo 18 aÃ±os, el cientÃ­fico y filÃ³sofo francÃ©s Blaise Pascal (1623-1666) inventÃ³ la primera calculadora mecÃ¡nica prÃ¡ctica , la Pascalina, para ayudar a su padre recaudador de impuestos a hacer sumas. La mÃ¡quina tenÃ­a una serie de dientes entrelazados (ruedas dentadas con dientes alrededor de sus bordes exteriores) que podÃ­an sumar y restar nÃºmeros decimales.

Varias dÃ©cadas despuÃ©s, en 1671, el matemÃ¡tico y filÃ³sofo alemÃ¡n Gottfried Wilhelm Leibniz(1646-1716) se le ocurriÃ³ una mÃ¡quina similar pero mÃ¡s avanzada.

En lugar de usar engranajes, tenÃ­a un "tambor escalonado" (un cilindro con dientes de longitud creciente alrededor de su borde), una innovaciÃ³n que sobreviviÃ³ en las calculadoras mecÃ¡nicas durante 300 aÃ±os. La mÃ¡quina Leibniz podrÃ­a hacer mucho mÃ¡s que la de Pascal: ademÃ¡s de sumar y restar, podrÃ­a multiplicar, dividir y resolver raÃ­ces cuadradas. Otra caracterÃ­stica pionera fue el primer almacÃ©n de memoria o "registro".

AdemÃ¡s de desarrollar una de las primeras calculadoras mecÃ¡nicas del mundo, Leibniz es recordado por otra contribuciÃ³n importante a la informÃ¡tica: fue el hombre que inventÃ³ el cÃ³digo binario, una forma de representar cualquier nÃºmero decimal usando solo los dos dÃ­gitos cero y uno. Aunque Leibniz no hizo uso del binario en su propia calculadora, hizo pensar a otros.

En 1854, poco mÃ¡s de un siglo despuÃ©s de la muerte de Leibniz, el inglÃ©s George Boole (1815-1864) utilizÃ³ la idea para inventar una nueva rama de las matemÃ¡ticas llamada Ã¡lgebra booleana.

En las computadoras modernas, el cÃ³digo binario y el Ã¡lgebra booleana permiten que las computadoras tomen decisiones simples al comparar largas cadenas de ceros y unos. Pero, en el siglo XIX, estas ideas todavÃ­a estaban muy adelantadas a su tiempo. A los matemÃ¡ticos e informÃ¡ticos les llevarÃ­a otros 50 a 100 aÃ±os descubrir cÃ³mo usarlos (descubra mÃ¡s en nuestros artÃ­culos sobre calculadoras y puertas lÃ³gicas ).

## 2. **Motores de cÃ¡lculo**

Ni el Ã¡baco ni las calculadoras mecÃ¡nicas construidas por Pascal y Leibniz realmente fueron calificadas como computadoras. Una calculadora es un dispositivo que facilita y agiliza el cÃ¡lculo de las personas, pero necesita un operador humano.

Una computadora, por otro lado, es una mÃ¡quina que puede operar automÃ¡ticamente, sin ayuda humana, siguiendo una serie de instrucciones almacenadas llamadas programa (una especie de receta matemÃ¡tica). Las calculadoras se convirtieron en computadoras cuando las personas idearon formas de hacer calculadoras programables y totalmente automÃ¡ticas.

CÃ³mo se usaban las tarjetas perforadas en las primeras computadoras.  Un dibujo de la Patente Art of Compiling Statistics de Herman Hollerith, 8 de enero de 1889.

Foto: Tarjetas perforadas: Herman Hollerith perfeccionÃ³ la forma de usar tarjetas perforadas y cinta de papel para almacenar informaciÃ³n y alimentarla en una mÃ¡quina. Existen dibujos de su Arte de compilaciÃ³n de estadÃ­sticas de 1889 (Patente de EE. UU. # 395,782), que muestra cÃ³mo se perfora una tira de papel (amarillo) con diferentes patrones de agujeros (naranja) que corresponden a las estadÃ­sticas recopiladas sobre las personas en el censo de EE. UU. Imagen cortesÃ­a de la Oficina de Patentes y Marcas de los Estados Unidos.

La primera persona en intentar esto fue un matemÃ¡tico inglÃ©s bastante obsesivo y notoriamente gruÃ±Ã³n llamado Charles Babbage (1791-1871). Muchos consideran a Babbage como el "padre de la computadora" porque sus mÃ¡quinas tenÃ­an una entrada (una forma de alimentar los nÃºmeros), una memoria (algo para almacenar estos nÃºmeros mientras se realizaban cÃ¡lculos complejos), un procesador (el generador de nÃºmeros que llevÃ³ a cabo los cÃ¡lculos) y una salida (un mecanismo de impresiÃ³n), los mismos componentes bÃ¡sicos compartidos por todas las computadoras modernas.

Durante su vida, Babbage nunca completÃ³ una sola de las mÃ¡quinas enormemente ambiciosas que intentÃ³ construir. Eso no fue una sorpresa. Cada uno de sus "motores" programables fue diseÃ±ado para usar decenas de miles de engranajes hechos con precisiÃ³n., una mÃ¡quina Pascal o Leibniz aumentaba mil veces en dimensiones, ambiciÃ³n y complejidad.

Durante un tiempo, el gobierno britÃ¡nico financiÃ³ Babbage, por un monto de Â£ 17,000, luego una suma enorme. Pero cuando Babbage presionÃ³ al gobierno para obtener mÃ¡s dinero para construir una mÃ¡quina aÃºn mÃ¡s avanzada, perdieron la paciencia y se retiraron. Babbage fue mÃ¡s afortunado al recibir ayuda de Augusta Ada Byron (1815-1852), condesa de Lovelace, hija del poeta Lord Byron.

Como matemÃ¡tica entusiasta, ayudÃ³ a refinar las ideas de Babbage para hacer que su mÃ¡quina fuera programable, y es por eso que a veces se la conoce como la primera programadora de computadoras del mundo. Poco del trabajo de Babbage sobreviviÃ³ despuÃ©s de su muerte. Pero cuando, por casualidad, sus cuadernos fueron redescubiertos en la dÃ©cada de 1930, los informÃ¡ticos finalmente apreciaron el brillo de sus ideas. Desafortunadamente, para entonces, la mayorÃ­a de estas ideas ya habÃ­an sido reinventadas por otros.

Babbage tenÃ­a la intenciÃ³n de que su mÃ¡quina eliminara el trabajo pesado de los cÃ¡lculos repetitivos. Originalmente, se imaginÃ³ que serÃ­a usado por el ejÃ©rcito para compilar las tablas que ayudaron a sus artilleros a disparar caÃ±ones con mayor precisiÃ³n.

Hacia finales del siglo XIX, otros inventores tuvieron mÃ¡s Ã©xito en su esfuerzo por construir "motores" de cÃ¡lculo. EstadÃ­stico estadounidense Herman Hollerith(1860â€“1929) construyeron una de las primeras mÃ¡quinas de cÃ¡lculo prÃ¡cticas del mundo, a la que llamÃ³ tabulador, para ayudar a compilar datos del censo.

Entonces, como ahora, se realizaba un censo cada dÃ©cada, pero, en la dÃ©cada de 1880, la poblaciÃ³n de los Estados Unidos habÃ­a crecido tanto a travÃ©s de la inmigraciÃ³n que un anÃ¡lisis a gran escala de los datos a mano llevaba siete aÃ±os y medio. Los estadÃ­sticos pronto descubrieron que, si las tendencias continuaban, se les agotarÃ­a el tiempo para compilar un censo antes de que venza el siguiente.

Afortunadamente, el tabulador de Hollerith fue un Ã©xito increÃ­ble: contÃ³ todo el censo en solo seis semanas y completÃ³ el anÃ¡lisis completo en solo dos aÃ±os y medio. Poco despuÃ©s, Hollerith se dio cuenta de que su mÃ¡quina tenÃ­a otras aplicaciones, por lo que creÃ³ la Tabulation Machine Company en 1896 para fabricarla comercialmente.

## 3. **Bush y la bomba**

La historia de la computaciÃ³n recuerda personajes coloridos como Babbage, pero otros que desempeÃ±aron roles importantes, aunque de apoyo, son menos conocidos. En el momento en que CTR se estaba convirtiendo en IBM, el cientÃ­fico del gobierno estadounidense Vannevar Bush (1890-1974) estaba desarrollando las calculadoras mÃ¡s potentes del mundo.

En 1925, Bush hizo el primero de una serie de artilugios difÃ­ciles de manejar con nombres igualmente engorrosos: el nuevo multiplicador de interacciones de productos de grabaciÃ³n. MÃ¡s tarde, construyÃ³ una mÃ¡quina llamada Analizador diferencial, que utilizaba engranajes, correas, palancas y ejes para representar nÃºmeros y realizar cÃ¡lculos de una manera muy fÃ­sica, como una gigantesca regla de cÃ¡lculo mecÃ¡nico.

La calculadora definitiva de Bush fue una mÃ¡quina mejorada llamada Analizador diferencial Rockefeller, ensamblada en 1935 a partir de 320 km (200 millas) de cable y 150motores electricos . Las mÃ¡quinas como estas se conocÃ­an como calculadoras analÃ³gicas , analÃ³gicas porque almacenaban nÃºmeros en forma fÃ­sica (como tantas vueltas en una rueda o giros de una correa) en lugar de dÃ­gitos. Aunque pudieron realizar cÃ¡lculos increÃ­blemente complejos, se necesitaron varios dÃ­as para hacer girar las ruedas y girar la correa antes de que finalmente aparecieran los resultados.

Impresionantes mÃ¡quinas como el Analizador Diferencial fueron solo una de varias contribuciones sobresalientes que Bush hizo a la tecnologÃ­a del siglo XX. Otro vino como el maestro de Claude Shannon (1916â€“2001), un matemÃ¡tico brillante que descubriÃ³ cÃ³mo los circuitos elÃ©ctricos podÃ­an vincularse para procesar cÃ³digo binario con Ã¡lgebra booleana (una forma de comparar nÃºmeros binarios usando la lÃ³gica) y asÃ­ tomar decisiones simples.

Durante la Segunda Guerra Mundial, el presidente Franklin D. Roosevelt nombrÃ³ al presidente de Bush primero del ComitÃ© de InvestigaciÃ³n de Defensa Nacional de los Estados Unidos y luego director de la Oficina de InvestigaciÃ³n y Desarrollo CientÃ­fico (OSRD).

En esta capacidad, estuvo a cargo del Proyecto Manhattan, la iniciativa secreta de 2 mil millones de dÃ³lares que condujo a la creaciÃ³n de la bomba atÃ³mica. Una de las contribuciones finales de Bush en tiempo de guerra fue esbozar, en 1945, una idea para un dispositivo de almacenamiento y uso compartido de memoria llamado Memex que luego inspirarÃ­a a Tim Berners-Lee a inventar la World Wide Web.

Pocos fuera del mundo de la informÃ¡tica recuerdan a Vannevar Bush hoy, Â¡pero quÃ© legado! Como padre de la computadora digital, un supervisor de la bomba atÃ³mica y una inspiraciÃ³n para la Web, Bush jugÃ³ un papel fundamental en tres de las tecnologÃ­as de mayor alcance del siglo XX.

## 4. **Turing: Probando**

Muchos de los pioneros de la informÃ¡tica fueron experimentadores prÃ¡cticos, pero de ninguna manera todos ellos. Una de las figuras clave en la historia de la informÃ¡tica del siglo XX, Alan Turing (1912â€“1954) fue un brillante matemÃ¡tico de Cambridge cuyas principales contribuciones fueron a la teorÃ­a.de cÃ³mo las computadoras procesaron la informaciÃ³n.

En 1936, a la edad de solo 23 aÃ±os, Turing escribiÃ³ un artÃ­culo matemÃ¡tico innovador llamado "Sobre nÃºmeros computables, con una aplicaciÃ³n al problema Entscheidungs", en el que describiÃ³ una computadora teÃ³rica ahora conocida como mÃ¡quina de Turing (un procesador de informaciÃ³n simple que funciona a travÃ©s de una serie de instrucciones, leer datos, escribir resultados y luego pasar a la siguiente instrucciÃ³n). Las ideas de Turing fueron muy influyentes en los aÃ±os siguientes y muchas personas lo consideran el padre de la informÃ¡tica moderna, el equivalente de Babbage del siglo XX.

Aunque esencialmente era un teÃ³rico, Turing se involucrÃ³ con maquinaria real y prÃ¡ctica, a diferencia de muchos matemÃ¡ticos de su tiempo. Durante la Segunda Guerra Mundial, desempeÃ±Ã³ un papel fundamental en el desarrollo de maquinaria para descifrar cÃ³digos que, en sÃ­ misma, desempeÃ±Ã³ un papel clave en la victoria britÃ¡nica en tiempos de guerra.

MÃ¡s tarde, jugÃ³ un papel menor en la creaciÃ³n de varias computadoras experimentales a gran escala, incluyendo ACE (Automatic Computing Engine), Colossus y el Manchester / Ferranti Mark I (descrito a continuaciÃ³n).

Hoy, Alan Turing es mÃ¡s conocido por concebir lo que se conoce como la prueba de Turing, una forma simple de averiguar si una computadora puede considerarse inteligente al comprobar si puede o no mantener una conversaciÃ³n plausible con un ser humano real.

## 5. **Las primeras computadoras modernas**

Los aÃ±os de la Segunda Guerra Mundial fueron un perÃ­odo crucial en la historia de la informÃ¡tica, cuando comenzaron a aparecer poderosas computadoras gigantes. Justo antes del estallido de la guerra, en 1938, el ingeniero alemÃ¡n Konrad Zuse (1910â€“1995) construyÃ³ su Z1, la primera computadora binaria programable del mundo, en la sala de estar de sus padres.

Al aÃ±o siguiente, el fÃ­sico estadounidense John Atanasoff (1903â€“1995) y su asistente, el ingeniero elÃ©ctrico Clifford Berry(1918â€“1963), construyeron una mÃ¡quina binaria mÃ¡s elaborada que llamaron Atanasoff Berry Computer (ABC).

Fue un gran avance, 1000 veces mÃ¡s preciso que el analizador diferencial de Bush. Estas fueron las primeras mÃ¡quinas que utilizaron interruptores elÃ©ctricos para almacenar nÃºmeros: cuando un interruptor estaba "apagado", almacenaba el nÃºmero cero; volteado a su otra posiciÃ³n, "encendido", almacenÃ³ el nÃºmero uno.

Cientos o miles de conmutadores podrÃ­an almacenar una gran cantidad de dÃ­gitos binarios (aunque el binario es mucho menos eficiente a este respecto que el decimal, ya que se necesitan hasta ocho dÃ­gitos binarios para almacenar un nÃºmero decimal de tres dÃ­gitos). Estas mÃ¡quinas eran computadoras digitales: a diferencia de las mÃ¡quinas analÃ³gicas, que almacenaban nÃºmeros usando las posiciones de ruedas y barras, almacenaban nÃºmeros como dÃ­gitos.

La primera computadora digital a gran escala de este tipo apareciÃ³ en 1944 en la Universidad de Harvard, construida por el matemÃ¡tico Howard Aiken (1900-1973). Patrocinado por IBM, se le conocÃ­a como Harvard Mark I o Calculadora automÃ¡tica de secuencia controlada de IBM (ASCC).

Una mÃ¡quina gigante, con una longitud de 15 m (50 pies), era como una enorme calculadora mecÃ¡nica integrada en una pared. Debe haber sonado impresionante, porque almacenaba y procesaba nÃºmeros usando relÃ©s electromagnÃ©ticos "clickety-clack" (imanes operados elÃ©ctricamente que cambiaban automÃ¡ticamente las lÃ­neas en el telÃ©fonointercambios): no menos de 3304 de ellos.

Puede que hayan sido impresionantes, pero los relevos sufrieron varios problemas: eran grandes (por eso el Harvard Mark I tenÃ­a que ser tan grande); necesitaban fuertes impulsos de poder para hacerlos cambiar; y eran lentos (tomÃ³ un tiempo para que un relÃ© cambiara de "apagado" a "encendido" o de 0 a 1).

La mayorÃ­a de las mÃ¡quinas desarrolladas en esta Ã©poca estaban destinadas a fines militares. Al igual que los motores mecÃ¡nicos nunca construidos de Babbage, fueron diseÃ±ados para calcular las mesas de tiro de artillerÃ­a y analizar las otras tareas complejas que entonces eran la gran cantidad de matemÃ¡ticos militares.

Durante la Segunda Guerra Mundial, los militares cooptaron a miles de las mejores mentes cientÃ­ficas: reconociendo que la ciencia ganarÃ­a la guerra, la Oficina de InvestigaciÃ³n y Desarrollo CientÃ­fico de Vannevar Bush empleÃ³ a 10,000 cientÃ­ficos solo de los Estados Unidos. Las cosas eran muy diferentes en Alemania. Cuando Konrad Zuse ofreciÃ³ construir su computadora Z2 para ayudar al ejÃ©rcito, no pudieron ver la necesidad y lo rechazaron.

Del lado aliado, las grandes mentes comenzaron a hacer grandes avances. En 1943, un equipo de matemÃ¡ticos con sede en Bletchley Park, cerca de Londres, Inglaterra (incluido Alan Turing) construyÃ³ una computadora llamada Coloso para ayudarlos a descifrar cÃ³digos secretos alemanes. Coloso fue la primera computadora totalmente electrÃ³nica.

En lugar de relÃ©s, utilizÃ³ una mejor forma de interruptor conocido como tubo de vacÃ­o (tambiÃ©n conocido, especialmente en Gran BretaÃ±a, como vÃ¡lvula). El tubo de vacÃ­o, cada uno tan grande como el pulgar de una persona y brillando al rojo vivo como una pequeÃ±a bombilla elÃ©ctrica, fue inventado en 1906 por Lee de Forest (1873-1961), quien lo llamÃ³ Audion.

Este avance le valiÃ³ a De Forest su sobrenombre como "el padre de la radio" porque su primer uso importante fue en receptores de radio , donde amplificaron seÃ±ales entrantes dÃ©biles para que las personas puedan escucharlas con mayor claridad.

En computadoras como ABC y Colossus, los tubos de vacÃ­o encontraron un uso alternativo como interruptores mÃ¡s rÃ¡pidos y compactos.

Al igual que los cÃ³digos que intentaba descifrar, Coloso era de alto secreto y su existencia no se confirmÃ³ hasta despuÃ©s de que terminÃ³ la guerra. En lo que respecta a la mayorÃ­a de las personas, los tubos de vacÃ­o fueron pioneros de una computadora mÃ¡s visible que apareciÃ³ en 1946: el Calculador e Integrador NumÃ©rico ElectrÃ³nico (ENIAC).

Los inventores de ENIAC, dos cientÃ­ficos de la Universidad de Pennsylvania, John Mauchly (1907â€“1980) y J. Presper Eckert (1919â€“1995), se inspiraron originalmente en el analizador diferencial de Bush; aÃ±os mÃ¡s tarde, Eckert recordÃ³ que ENIAC era el "descendiente de la mÃ¡quina del Dr. Bush".

Pero la mÃ¡quina que construyeron era mucho mÃ¡s ambiciosa. ContenÃ­a cerca de 18,000 tubos de vacÃ­o (nueve veces mÃ¡s que Coloso), medÃ­a alrededor de 24 m (80 pies) de largo y pesaba casi 30 toneladas. ENIAC es generalmente reconocida como la primera computadora digital totalmente electrÃ³nica y de propÃ³sito general del mundo.

Coloso tambiÃ©n podrÃ­a haber calificado para este tÃ­tulo, pero fue diseÃ±ado exclusivamente para un trabajo (descifrado de cÃ³digo); Como no podÃ­a almacenar un programa, no podÃ­a reprogramarse fÃ¡cilmente para hacer otras cosas.

ENIAC fue solo el comienzo. Sus dos inventores formaron la Eckert Mauchly Computer Corporation a fines de la dÃ©cada de 1940. Trabajando con un brillante matemÃ¡tico hÃºngaro, John von Neumann (1903â€“1957), con sede en la Universidad de Princeton, diseÃ±aron una mÃ¡quina mejor llamada EDVAC (Electronic Discrete Variable Automatic Computer). En un trabajo clave, von Neumann ayudÃ³ a definir cÃ³mo la mÃ¡quina almacenaba y procesaba sus programas, sentando las bases de cÃ³mo funcionan todas las computadoras modernas. [6] DespuÃ©s de EDVAC, Eckert y Mauchly desarrollaron UNIVAC 1 (Computadora automÃ¡tica UNIVersal) en 1951.

Fueron ayudados en esta tarea por una joven matemÃ¡tica y reserva naval estadounidense en gran parte desconocida llamada Grace Murray Hopper (1906-1992), quien originalmente empleado por Howard Aiken en el Harvard Mark I. Al igual que el tabulador de Herman Hollerith mÃ¡s de 50 aÃ±os antes, UNIVAC 1 se utilizÃ³ para procesar datos del censo de EE. UU. Luego se fabricÃ³ para otros usuarios y se convirtiÃ³ en la primera computadora comercial a gran escala del mundo.

MÃ¡quinas como Colossus, ENIAC y Harvard Mark I compiten por el significado y el reconocimiento en la mente de los historiadores de la informÃ¡tica. Â¿CuÃ¡l fue realmente la primera gran computadora moderna? Todos y ninguno: estas, y varias otras mÃ¡quinas importantes, evolucionaron nuestra idea de la computadora electrÃ³nica moderna durante el perÃ­odo clave entre finales de la dÃ©cada de 1930 y principios de la dÃ©cada de 1950.

Entre esas otras mÃ¡quinas habÃ­a computadoras pioneras creadas por acadÃ©micos ingleses, en particular la Manchester / Ferranti Mark I, construida en la Universidad de Manchester por Frederic Williams (1911â€“1977) y Thomas Kilburn (1921â€“2001), y la EDSAC (Electronic Delay Storage Automatic Calculadora), construida por Maurice Wilkes (1913â€“2010) en la Universidad de Cambridge.

## 6. **La revoluciÃ³n micro electrÃ³nica**

Los tubos de vacÃ­o fueron un avance considerable en los interruptores de relÃ©, pero las mÃ¡quinas como la ENIAC eran notoriamente poco confiables. El tÃ©rmino moderno para un problema que detiene un programa de computadora es un "error".

La leyenda popular dice que esta palabra entrÃ³ en el vocabulario de los programadores de computadoras en algÃºn momento de la dÃ©cada de 1950 cuando las polillas, atraÃ­das por las luces brillantes de los tubos de vacÃ­o, volaron dentro de mÃ¡quinas como el ENIAC, causaron un corto circuito y detuvieron el trabajo. Pero tambiÃ©n hubo otros problemas con los tubos de vacÃ­o. Consumieron enormes cantidades de energÃ­a: el ENIAC usaba aproximadamente 2000 veces mÃ¡s electricidad que una computadora portÃ¡til moderna. Y ocuparon grandes cantidades de espacio.

Las necesidades militares estaban impulsando el desarrollo de mÃ¡quinas como la ENIAC, pero el gran tamaÃ±o de los tubos de vacÃ­o ahora se habÃ­a convertido en un problema real. ABC habÃ­a usado 300 tubos de vacÃ­o, Coloso tenÃ­a 2000, y el ENIAC tenÃ­a 18,000. Los diseÃ±adores de ENIAC se habÃ­an jactado de que su velocidad de cÃ¡lculo era "al menos 500 veces mayor que la de cualquier otra mÃ¡quina informÃ¡tica existente".

Pero el desarrollo de computadoras que eran de un orden de magnitud mÃ¡s potente aÃºn habrÃ­a necesitado cientos de miles o incluso millones de tubos de vacÃ­o, lo que habrÃ­a sido demasiado costoso, difÃ­cil de manejar y poco confiable. Por lo tanto, se requerÃ­a urgentemente una nueva tecnologÃ­a.

La soluciÃ³n apareciÃ³ en 1947 gracias a tres fÃ­sicos que trabajan en los Laboratorios Bell Telephone (Bell Labs). John Bardeen (1908â€“1991), Walter Brattain (1902â€“1987) y William Shockley (1910â€“1989) estaban ayudando a Bell a desarrollar una nueva tecnologÃ­a para el sistema telefÃ³nico pÃºblico estadounidense, por lo que las seÃ±ales elÃ©ctricas que transmitÃ­an las llamadas telefÃ³nicas podÃ­an amplificarse mÃ¡s fÃ¡cilmente y llevado mÃ¡s lejos.

Shockley, quien lideraba el equipo, creÃ­a que podÃ­a usar semiconductores (materiales como germanio y silicio que permiten que la electricidad fluya a travÃ©s de ellos solo cuando han sido tratados de manera especial) para hacer una mejor forma de amplificadorque el tubo de vacÃ­o Cuando sus primeros experimentos fallaron, hizo que Bardeen y Brattain trabajaran en la tarea por Ã©l.

Finalmente, en diciembre de 1947, crearon una nueva forma de amplificador que se conociÃ³ como el transistor de contacto de punto. Bell Labs acreditÃ³ a Bardeen y Brattain con el transistor y les otorgÃ³ una patente. Esto enfureciÃ³ a Shockley y lo impulsÃ³ a inventar un diseÃ±o aÃºn mejor, el transistor de uniÃ³n, que ha formado la base de la mayorÃ­a de los transistores desde entonces.

Al igual que los tubos de vacÃ­o, los transistores podrÃ­an usarse como amplificadores o interruptores. Pero tenÃ­an varias ventajas importantes. Eran una fracciÃ³n del tamaÃ±o de los tubos de vacÃ­o (generalmente del tamaÃ±o de un guisante), no usaban energÃ­a a menos que estuvieran en funcionamiento y eran prÃ¡cticamente 100 por ciento confiables.

El transistor fue uno de los avances mÃ¡s importantes en la historia de la informÃ¡tica y le valiÃ³ a sus inventores el mayor premio de ciencias del mundo, el Premio Nobel de FÃ­sica de 1956 . Para entonces, sin embargo, los tres hombres ya se habÃ­an separado. John Bardeen habÃ­a comenzado la investigaciÃ³n pionera en superconductividad , lo que le harÃ­a ganar un segundo Premio Nobel en 1972. Walter Brattain se mudÃ³ a otra parte de los Laboratorios Bell.

William Shockley decidiÃ³ quedarse con el transistor, eventualmente formando su propia corporaciÃ³n para desarrollarlo aÃºn mÃ¡s. Su decisiÃ³n tendrÃ­a consecuencias extraordinarias para la industria informÃ¡tica. Con una pequeÃ±a cantidad de capital, Shockley se propuso contratar los mejores cerebros que pudo encontrar en las universidades estadounidenses, incluido el joven ingeniero elÃ©ctrico Robert Noyce (1927â€“1990) y el quÃ­mico de investigaciÃ³n Gordon Moore(1929â€“). No pasÃ³ mucho tiempo antes de que el estilo de gestiÃ³n idiosincrÃ¡sico y de intimidaciÃ³n de Shockley molestara a sus trabajadores.

En 1956, ocho de ellos, incluidos Noyce y Moore, dejaron el transistor Shockley para fundar una compaÃ±Ã­a propia, Fairchild Semiconductor, justo en el camino. AsÃ­ comenzÃ³ el crecimiento de "Silicon Valley", la parte de California centrada en Palo Alto, donde desde entonces se han basado muchas de las principales empresas de informÃ¡tica y electrÃ³nica del mundo. [8]

Fue en el edificio de Fairchild en California donde ocurriÃ³ el siguiente avance, aunque, curiosamente, tambiÃ©n ocurriÃ³ exactamente al mismo tiempo en los laboratorios de Dallas Instruments de Texas Instruments. En Dallas, un joven ingeniero de Kansas llamado Jack Kilby (1923â€“2005) estaba considerando cÃ³mo mejorar el transistor.

Aunque los transistores fueron un gran avance en los tubos de vacÃ­o, quedaba un problema clave. Las mÃ¡quinas que usaban miles de transistores aÃºn tenÃ­an que conectarse manualmente para conectar todos estos componentes. Ese proceso fue laborioso, costoso y propenso a errores.

Â¿No serÃ­a mejor, reflexionÃ³ Kilby, si se pudieran hacer muchos transistores en un solo paquete?

Esto lo llevÃ³ a inventar el circuito integrado "monolÃ­tico" (IC), una colecciÃ³n de transistores y otros componentes que podrÃ­an fabricarse de una vez, en un bloque, en la superficie de un semiconductor. El invento de Kilby fue otro paso adelante, pero tambiÃ©n tenÃ­a un inconveniente: los componentes de su circuito integrado todavÃ­a tenÃ­an que conectarse a mano.

Mientras Kilby estaba haciendo su gran avance en Dallas, desconocido para Ã©l, Robert Noyce estaba perfeccionando casi exactamente la misma idea en Fairchild en California. Sin embargo, Noyce fue mejor: encontrÃ³ una manera de incluir las conexiones entre componentes en un circuito integrado, automatizando asÃ­ todo el proceso.

Los circuitos integrados, tanto como los transistores, ayudaron a reducir las computadoras durante la dÃ©cada de 1960. En 1943, el jefe de IBM, Thomas Watson, habÃ­a bromeado: "Creo que hay un mercado mundial para unas cinco computadoras".

Solo dos dÃ©cadas despuÃ©s, la compaÃ±Ã­a y sus competidores habÃ­an instalado alrededor de 25,000 grandes sistemas informÃ¡ticos en todo Estados Unidos. A medida que avanzaba la dÃ©cada de 1960, los circuitos integrados se volvieron cada vez mÃ¡s sofisticados y compactos. Pronto, los ingenieros hablaron de la integraciÃ³n a gran escala (LSI), en la que cientos de componentes podrÃ­an agruparse en un solo chip, y luego integrarse a muy gran escala (VLSI), cuando el mismo chip podrÃ­a contener miles de componentes.

La conclusiÃ³n lÃ³gica de toda esta miniaturizaciÃ³n fue que, algÃºn dÃ­a, alguien podrÃ­a exprimir una computadora completa en un chip. En 1968, Robert Noyce y Gordon Moore habÃ­an dejado Fairchild para establecer una nueva compaÃ±Ã­a propia.

Con mucha integraciÃ³n en sus mentes, lo llamaron Integrated Electronics o Intel para abreviar. Originalmente habÃ­an planeado hacer chips de memoria, pero cuando la compaÃ±Ã­a obtuvo una orden para hacer chips para una gama de calculadoras de bolsillo, la historia se dirigiÃ³ en una direcciÃ³n diferente. Un par de sus ingenieros, Federico Faggin (1941â€“) y Marcian Edward (Ted) Hoff(1937â€“), se dieron cuenta de que en lugar de hacer una gama de chips especializados para una gama de calculadoras, podrÃ­an hacer un chip universal que pudiera programarse para funcionar en todos ellos.

AsÃ­ naciÃ³ la computadora de un solo chip de uso general o el microprocesador, y eso provocÃ³ la siguiente fase de la revoluciÃ³n de la computadora.

## 7. **Computadoras personales**

Los circuitos integrados, tanto como los transistores, ayudaron a reducir las computadoras durante la dÃ©cada de 1960. En 1943, el jefe de IBM, Thomas Watson, habÃ­a bromeado: "Creo que hay un mercado mundial para unas cinco computadoras".

Solo dos dÃ©cadas despuÃ©s, la compaÃ±Ã­a y sus competidores habÃ­an instalado alrededor de 25,000 grandes sistemas informÃ¡ticos en todo Estados Unidos. A medida que avanzaba la dÃ©cada de 1960, los circuitos integrados se volvieron cada vez mÃ¡s sofisticados y compactos. Pronto, los ingenieros hablaron de la integraciÃ³n a gran escala (LSI), en la que cientos de componentes podrÃ­an agruparse en un solo chip, y luego integrarse a muy gran escala (VLSI), cuando el mismo chip podrÃ­a contener miles de componentes.

La conclusiÃ³n lÃ³gica de toda esta miniaturizaciÃ³n fue que, algÃºn dÃ­a, alguien podrÃ­a exprimir una computadora completa en un chip. En 1968, Robert Noyce y Gordon Moore habÃ­an dejado Fairchild para establecer una nueva compaÃ±Ã­a propia.

Con mucha integraciÃ³n en sus mentes, lo llamaron Integrated Electronics o Intel para abreviar. Originalmente habÃ­an planeado hacer chips de memoria, pero cuando la compaÃ±Ã­a obtuvo una orden para hacer chips para una gama de calculadoras de bolsillo, la historia se dirigiÃ³ en una direcciÃ³n diferente.

Un par de sus ingenieros, Federico Faggin (1941â€“) y Marcian Edward (Ted) Hoff(1937â€“), se dieron cuenta de que en lugar de hacer una gama de chips especializados para una gama de calculadoras, podrÃ­an hacer un chip universal que pudiera programarse para funcionar en todos ellos. AsÃ­ naciÃ³ la computadora de un solo chip de uso general o el microprocesador, y eso provocÃ³ la siguiente fase de la revoluciÃ³n de la computadora.

El Ã©xito de la venta de Apple a las empresas fue un gran shock para IBM y las otras grandes empresas que dominaron la industria informÃ¡tica. No se necesitÃ³ una hoja de cÃ¡lculo de VisiCalc para darse cuenta de que, si la tendencia continuaba, empresas nuevas como Apple socavarÃ­an el inmensamente lucrativo mercado comercial de computadoras "Big Blue" de IBM. En 1980, IBM finalmente se dio cuenta de que tenÃ­a que hacer algo y lanzÃ³ un proyecto altamente racionalizado para salvar su negocio. Un aÃ±o despuÃ©s, lanzÃ³ la IBM Personal Computer (PC), basada en un microprocesador Intel 8080, que revirtiÃ³ rÃ¡pidamente la fortuna de la compaÃ±Ã­a y robÃ³ el mercado a Apple.

La PC tuvo Ã©xito esencialmente por una razÃ³n. Todas las docenas de microcomputadoras que se lanzaron en la dÃ©cada de 1970, incluida Apple] [- eran incompatibles. Todos usaron hardware diferente y trabajaron de diferentes maneras. La mayorÃ­a se programaron usando un lenguaje simple, similar al inglÃ©s llamado BASIC, pero cada uno usÃ³ su propio sabor de BASIC, que estaba estrechamente relacionado con el diseÃ±o del hardware de la mÃ¡quina.

Como resultado, los programas escritos para una mÃ¡quina generalmente no se ejecutarÃ­an en otra sin una gran conversiÃ³n. Las empresas que escribieron software profesionalmente lo escribieron solo para una mÃ¡quina y, en consecuencia, no habÃ­a industria del software para hablar.

En 1976, Gary Kildall(1942â€“1994), profesor e informÃ¡tico, y uno de los fundadores del Homebrew Computer Club, habÃ­a encontrado una soluciÃ³n a este problema. Kildall escribiÃ³ un sistema operativo (software de control fundamental de una computadora) llamado CP / M que actuÃ³ como intermediario entre los programas del usuario y el hardware de la mÃ¡quina.

Con un golpe de genio, Kildall se dio cuenta de que todo lo que tenÃ­a que hacer era reescribir CP / M para que funcionara en cada mÃ¡quina diferente. Entonces, todas esas mÃ¡quinas podrÃ­an ejecutar programas de usuario idÃ©nticos, sin ninguna modificaciÃ³n, dentro de CP / M. Eso harÃ­a que todos los diferentes microordenadores sean compatibles de un solo golpe. A principios de la dÃ©cada de 1980, Kildall se habÃ­a convertido en multimillonario gracias al Ã©xito de su invenciÃ³n: el primer sistema operativo de computadora personal. Naturalmente, cuando IBM estaba desarrollando su computadora personal, se le acercÃ³ esperando poner CP / M en su propia mÃ¡quina.

La leyenda dice que Kildall estaba volando su aviÃ³n personal cuando IBM llamÃ³, por lo que se perdiÃ³ una de las mejores ofertas del mundo. Pero la verdad parece haber sido que IBM querÃ­a comprar CP / M directamente por solo $ 200,000, mientras que Kildall reconociÃ³ que su producto valÃ­a millones mÃ¡s y se negÃ³ a venderlo. En cambio, IBM recurriÃ³ a un joven programador llamadoBill Gates (1955â€“). Su entonces pequeÃ±a empresa, Microsoft, creÃ³ rÃ¡pidamente un sistema operativo llamado DOS, basado en un producto llamado QDOS (Sistema operativo rÃ¡pido y sucio), que adquirieron de Seattle Computer Products.

Algunos creen que Microsoft e IBM engaÃ±aron a Kildall de su lugar en la historia de la computadora; El propio Kildall los acusÃ³ de copiar sus ideas. Otros piensan que Gates era simplemente el hombre de negocios mÃ¡s astuto. De cualquier manera, la PC IBM, impulsada por el sistema operativo de Microsoft, fue un gran Ã©xito.

Sin embargo, la victoria de IBM fue de corta duraciÃ³n. Curiosamente, Bill Gates habÃ­a vendido a IBM los derechos de una versiÃ³n de DOS (PC-DOS) y retuvo los derechos de una versiÃ³n muy similar (MS-DOS) para su propio uso.

Cuando otros fabricantes de computadoras, especialmente Compaq y Dell, comenzaron a fabricar hardware compatible con IBM (o "clonado"), tambiÃ©n acudieron a Gates para obtener el software. IBM cobraba una prima por las mÃ¡quinas que llevaban su insignia, pero los consumidores pronto se dieron cuenta de que las PC eran productos bÃ¡sicos: contenÃ­an componentes casi idÃ©nticos, un microprocesador Intel, por ejemplo, sin importar el nombre que tenÃ­an en el caso. A medida que IBM perdiÃ³ cuota de mercado, los Ãºltimos vencedores fueron Microsoft e Intel, que pronto suministraron el software y el hardware para casi todas las PC del planeta. Apple, IBM.

## 8. **La revoluciÃ³n del usuario**

Afortunadamente para Apple, tuvo otra gran idea. Uno de los trajes mÃ¡s fuertes del Apple II fue su simple "facilidad de uso". Para Steve Jobs, desarrollar computadoras verdaderamente fÃ¡ciles de usar se convirtiÃ³ en una misiÃ³n personal a principios de la dÃ©cada de 1980. Lo que realmente lo inspirÃ³ fue una visita al PARC (Centro de InvestigaciÃ³n de Palo Alto), un laboratorio informÃ¡tico de vanguardia que luego funcionaba como una divisiÃ³n de la CorporaciÃ³n Xerox. Xerox habÃ­a comenzado a desarrollar computadoras a principios de la dÃ©cada de 1970, creyendo que harÃ­an papel (y las fotocopiadoras altamente lucrativas)Xerox hecho) obsoleto.

Uno de los proyectos de investigaciÃ³n de PARC fue una computadora avanzada de $ 40,000 llamada Xerox Alto. A diferencia de la mayorÃ­a de las microcomputadoras lanzadas en la dÃ©cada de 1970, que se programaron escribiendo comandos de texto, el Alto tenÃ­a una pantalla similar al escritorio con pequeÃ±os Ã­conos de imÃ¡genes que se podÃ­an mover con un mouse: era la primera interfaz grÃ¡fica de usuario (GUI, pronunciada "pegajoso") - una idea concebida por Alan Kay (1940â€“) y que ahora se usa en prÃ¡cticamente todas las computadoras modernas. El Alto tomÃ³ prestadas algunas de sus ideas, incluido el mouse , del pionero informÃ¡tico de los aÃ±os sesenta Douglas Engelbart (1925â€“2013).

De vuelta en Apple, Jobs lanzÃ³ su propia versiÃ³n del proyecto Alto para desarrollar una computadora fÃ¡cil de usar llamada PITS (Person In The Street). Esta mÃ¡quina se convirtiÃ³ en Apple Lisa, lanzada en enero de 1983, la primera computadora ampliamente disponible con un escritorio GUI. Con un precio minorista de $ 10,000, mÃ¡s del triple del costo de una PC IBM, la Lisa fue un fracaso comercial.

Pero allanÃ³ el camino para una mÃ¡quina mejor y mÃ¡s barata llamada Macintosh que Jobs presentÃ³ un aÃ±o despuÃ©s, en enero de 1984. Con su memorable anuncio de lanzamiento para Macintosh inspirado en la novela 1984 de George Orwell y dirigida por Ridley Scott (director del distÃ³pico) pelÃ­cula Blade Runner), Apple golpeÃ³ el monopolio de IBM, criticando lo que describiÃ³ como el enfoque dominante, incluso totalitario, de la empresa: Big Blue era realmente Big Brother. 

El anuncio de Apple prometÃ­a una visiÃ³n muy diferente: "El 24 de enero, Apple Computer presentarÃ¡ Macintosh. Y verÃ¡ por quÃ© 1984 no serÃ¡ como '1984'". El Macintosh fue un Ã©xito crÃ­tico y ayudÃ³ a inventar el nuevo campo de la ediciÃ³n de escritorio a mediados de la dÃ©cada de 1980, sin embargo, nunca estuvo cerca de desafiar la posiciÃ³n de IBM.

IrÃ³nicamente, la mÃ¡quina fÃ¡cil de usar de Jobs tambiÃ©n ayudÃ³ a Microsoft a desalojar a IBM como la fuerza lÃ­der mundial en informÃ¡tica. Cuando Bill Gates vio cÃ³mo funcionaba el Macintosh, con su escritorio de icono de imagen fÃ¡cil de usar, lanzÃ³ Windows, una versiÃ³n mejorada de su software MS-DOS.

Apple vio esto como un plagio flagrante y presentÃ³ una demanda por derechos de autor de $ 5,5 mil millones en 1988. Cuatro aÃ±os mÃ¡s tarde, el caso colapsÃ³ con Microsoft asegurando efectivamente el derecho a usar el "aspecto" de Macintosh en todas las versiones actuales y futuras de Windows. El sistema Windows 95 de Microsoft, lanzado tres aÃ±os despuÃ©s, tenÃ­a un escritorio fÃ¡cil de usar, similar a Macintosh y MS-DOS ejecutÃ¡ndose detrÃ¡s de escena.

## 9. **De las redes a internet**

Las PC estandarizadas que ejecutan software estandarizado trajeron un gran beneficio para las empresas: las computadoras podÃ­an conectarse en redes para compartir informaciÃ³n. En Xerox PARC en 1973, el ingeniero elÃ©ctrico Bob Metcalfe (1946â€“) desarrollÃ³ una nueva forma de vincular computadoras "a travÃ©s del Ã©ter" (espacio vacÃ­o) que llamÃ³ Ethernet.

Unos aÃ±os mÃ¡s tarde, Metcalfe dejÃ³ Xerox para formar su propia compaÃ±Ã­a, 3Com, para ayudar a las empresas a cumplir la "Ley de Metcalfe": las computadoras se vuelven Ãºtiles cuanto mÃ¡s conectadas estÃ¡n con las computadoras de otras personas. A medida que mÃ¡s y mÃ¡s empresas exploran el poder de las redes de Ã¡rea local (LAN), a medida que avanza la dÃ©cada de 1980.

Hoy, la WAN mÃ¡s conocida es Internet , una red global de computadoras y LAN individuales que conecta a cientos de millones de personas. La historia de Internet es otra historia, pero comenzÃ³ en la dÃ©cada de 1960 cuando cuatro universidades estadounidenses lanzaron un proyecto para conectar sus sistemas informÃ¡ticos para crear la primera WAN.

MÃ¡s tarde, con fondos para el Departamento de Defensa, esa red se convirtiÃ³ en un proyecto mÃ¡s grande llamado ARPANET (Red de Agencias de Proyectos de InvestigaciÃ³n Avanzada). A mediados de la dÃ©cada de 1980, la FundaciÃ³n Nacional de Ciencias de EE. UU. (NSF) lanzÃ³ su propia WAN llamada NSFNET. La convergencia de todas estas redes produjo lo que ahora llamamos Internet mÃ¡s adelante en la dÃ©cada de 1980.

Poco despuÃ©s, el poder de las redes le dio al programador informÃ¡tico britÃ¡nico Tim Berners-Lee(1955â€“) su gran idea: combinar el poder de las redes de computadoras con la idea de compartir informaciÃ³n que Vannevar Bush habÃ­a propuesto en 1945.

AsÃ­ naciÃ³ la World Wide Web , una manera fÃ¡cil de compartir informaciÃ³n a travÃ©s de una red de computadoras, lo que hizo posible la era moderna de la computaciÃ³n en la nube (donde cualquiera puede acceder a una gran potencia informÃ¡tica a travÃ©s de Internet sin tener que preocuparse por dÃ³nde o cÃ³mo se procesan sus datos). Â¡Es el invento de Tim Berners-Lee el que te trae esta historia de la informÃ¡tica en macetas hoy!

## 10. **Â¿QuÃ© nos espera en el futuro?**

**Â¿QuÃ© hay del futuro?** El poder de las computadoras (la cantidad de componentes empaquetados en un chip) se ha duplicado aproximadamente cada 18 meses a 2 aÃ±os desde la dÃ©cada de 1960. Pero se espera que las leyes de la fÃ­sica detengan la Ley de Moore , como se conoce esta idea, y nos obligan a explorar formas completamente nuevas de construir computadoras.

**Â¿CÃ³mo serÃ¡n las PC del maÃ±ana?** Una idea muy esperada es que usarÃ¡n partÃ­culas de luz (fotones) en lugar de electrones, un enfoque conocido como computaciÃ³n Ã³ptica o fotÃ³nica. Actualmente, gran parte del dinero inteligente estÃ¡ apostando por computadoras cuÃ¡nticas , que implementan formas astutas de manipular Ã¡tomos para procesar y almacenar informaciÃ³n a la velocidad del rayo.

TambiÃ©n hay esperanza de que podamos usar spintronics (aprovechando el "giro" de las partÃ­culas) y tecnologÃ­a biomolecular (computaciÃ³n con ADN, proteÃ­nas y otras molÃ©culas biolÃ³gicas), aunque ambas se encuentran en las primeras etapas de la investigaciÃ³n.

Las virutas hechas de nuevos materiales como el grafeno tambiÃ©n pueden ofrecer formas de extender la ley de Moore. Independientemente de la tecnologÃ­a que gane, Â¡puede estar seguro de que el futuro de la informÃ¡tica serÃ¡ tan emocionante como el pasado!

> ğŸ”¥ Seguro tambiÃ©n te interesa: [CÃ³mo aprender Python en 2020](/python/), [ğŸ¥‡ â–· CÃ³mo aprender aprendizaje automÃ¡tico o machine learning en 2020 ğŸ¤–](/que-aprender-sobre-machine-learning-2020/), [â–· MÃ¡s de 200 de los mejores tutoriales de aprendizaje automÃ¡tico, PNL y Python](/aprendizaje-automatico-cursos-ingles/)
{: .notice--danger}

### Relacionados



<div class="fb-post" data-href="https://www.facebook.com/ciberninjas/posts/1331109157075936" data-width="850" data-show-text="true"><blockquote cite="https://developers.facebook.com/ciberninjas/posts/1331109157075936" class="fb-xfbml-parse-ignore"><p>ğŸ‘¨â€ğŸ’» Los mejores libros con los que aprender a programar en Java y con Android, en EspaÃ±ol</p>Publicada por <a href="https://www.facebook.com/ciberninjas/">Ciberninjas</a> en&nbsp;<a href="https://developers.facebook.com/ciberninjas/posts/1331109157075936">Martes, 3 de marzo de 2020</a></blockquote></div>
