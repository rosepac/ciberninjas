---
author_profile: true
title: "\U0001F916 Generador de Texto a travÃ©s de IA: Modelado de Lenguaje a Gran Escala"
description: >-
  ActualizaciÃ³n dentro del proyecto de Inteligencia Artificial capaz de predecir
  y generar texto de forma correcta simulando ser un humano.
excerpt: >-
  ActualizaciÃ³n dentro del proyecto de Inteligencia Artificial capaz de predecir
  y generar texto de forma correcta simulando ser un humano.
comments: false
date: 2019-09-01 10:30:00 +0200
tags:
  - Modelo
  - Lenguaje
  - AutomÃ¡tico
categories:
  - Inteligencia Artificial
---

Desde Open AI lanzan informaci&oacute;n sobre como avanza su proyecto de [Lenguaje Mejorado](https://openai.com/blog/better-language-models/#update). A groso modo, la historia trata de crear un generador de texto, que partiendo de una gran base de datos de textos recopilados a trav&eacute;s de Internet, entre otros sitios de Reddit; la computadora sea capaz de generar palabras con "sentido humano" en consonancia a lo que halla podido ir "aprendiendo".

No soy ning&uacute;n -pro- en la materia, perd&oacute;n por mis errores.

Ellos lo definen como: "Entrenamos un modelo de lenguaje no supervisado a gran escala que genera p&aacute;rrafos de texto coherentes, logra un rendimiento de vanguardia en muchos puntos de referencia de modelado de lenguaje y realiza una comprensi&oacute;n de lectura rudimentaria, traducci&oacute;n autom&aacute;tica, respuesta a preguntas y resumen, todo sin entrenamiento de tareas espec&iacute;ficas."

Un poco m&aacute;s del proyecto, GPT-2 muestra un amplio conjunto de capacidades, incluida la capacidad de generar muestras de texto sint&eacute;tico condicional de calidad sin precedentes, donde imprimamos el modelo con una entrada y hacemos que genere una larga continuaci&oacute;n. Adem&aacute;s, GPT-2 supera a otros modelos de idiomas capacitados en dominios espec&iacute;ficos (como Wikipedia, noticias o libros) sin necesidad de utilizar estos conjuntos de datos de capacitaci&oacute;n espec&iacute;ficos de dominio. En tareas de lenguaje como respuesta a preguntas, comprensi&oacute;n de lectura, resumen y traducci&oacute;n, GPT-2 comienza a aprender estas tareas a partir del texto sin procesar, sin utilizar datos de capacitaci&oacute;n espec&iacute;ficos de la tarea. Si bien las puntuaciones en estas tareas posteriores distan mucho de ser avanzadas, sugieren que las tareas pueden beneficiarse de t&eacute;cnicas no supervisadas, dados datos y c&aacute;lculos suficientes (sin etiqueta).<br><br>Si alguien desea investigar m&aacute;s en profundidad, para eso tenemos la fuente.. A mi todo esto me queda demasiado grande. ðŸ˜¹

**Fuente**\: [https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)
{: .notice--info}